{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10971755,"sourceType":"datasetVersion","datasetId":6827064},{"sourceId":10971920,"sourceType":"datasetVersion","datasetId":6827185},{"sourceId":10972326,"sourceType":"datasetVersion","datasetId":6827489},{"sourceId":10972766,"sourceType":"datasetVersion","datasetId":6827809},{"sourceId":10972970,"sourceType":"datasetVersion","datasetId":6827957}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport joblib  # for saving/loading models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.compose import ColumnTransformer\n\n# 1. Load Data\ndf = pd.read_csv(\"/kaggle/input/complete-dataset/Q2_dataset_complete.csv\")\n\n# Identify and remove rare classes (classes with only one sample)\nclass_counts = df['Q2_Topics'].value_counts()\nrare_classes = class_counts[class_counts == 1].index\ndf = df[~df['Q2_Topics'].isin(rare_classes)]\n\n# 2. Feature Engineering\nlabel_encoder = LabelEncoder()\ndf['Q2_Topics'] = df['Q2_Topics'].astype(str)\ndf['Q2_Topic_Encoded'] = label_encoder.fit_transform(df['Q2_Topics'])\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(handle_unknown='ignore'), ['Paper_Session'])],\n    remainder='passthrough'\n)\n\nX = df[['Year', 'Paper_Session', 'Paper_Varient']]\nX.columns = X.columns.astype(str)\nX_transformed = ct.fit_transform(X)  # fit_transform for training data\ny = df['Q2_Topic_Encoded']\n\n# 3. Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 4. Model Training\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Evaluation\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# 6. Prediction for Next Year (example)\nnext_year_data = pd.DataFrame({\n    'Year': [2025],\n    'Paper_Session': ['MJ'],\n    'Paper_Varient': ['12']\n})\n# We must transform it with the same ColumnTransformer\nX_next_year = ct.transform(next_year_data)\npredicted_topic_encoded = model.predict(X_next_year)\npredicted_topic = label_encoder.inverse_transform(predicted_topic_encoded)\nprint(f\"Predicted Topic for 2025: {predicted_topic[0]}\")\n\n# 7. SAVE EVERYTHING\n# We'll save: the trained model, the column transformer, and the label encoder\njoblib.dump(model, \"rf_model.pkl\")            # saves the RandomForest\njoblib.dump(ct, \"column_transformer.pkl\")     # saves the ColumnTransformer\njoblib.dump(label_encoder, \"label_encoder.pkl\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T20:01:58.839406Z","iopub.execute_input":"2025-03-09T20:01:58.839705Z","iopub.status.idle":"2025-03-09T20:02:00.207646Z","shell.execute_reply.started":"2025-03-09T20:01:58.839680Z","shell.execute_reply":"2025-03-09T20:02:00.206929Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.400\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         1\n           1       0.50      0.50      0.50         2\n           2       0.50      1.00      0.67         1\n           3       0.00      0.00      0.00         1\n\n    accuracy                           0.40         5\n   macro avg       0.25      0.38      0.29         5\nweighted avg       0.30      0.40      0.33         5\n\nPredicted Topic for 2025: the account of the compilation of the Qurâ€™an under the Rightly Guided Caliphs \n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['label_encoder.pkl']"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets imbalanced-learn difflib2 --quiet\n\nimport pandas as pd\nimport torch\nimport difflib\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments\n)\n\n#############################\n# 1) Load & Combine Dataset\n#############################\ndf = pd.read_csv(\"/kaggle/input/fazul-dataset/Q2_dataset_fazul.csv\")\ndf = df.dropna(subset=[\"Q2_Topic\", \"Q2\"])  # remove any empty rows\n\n# We'll store all existing questions in a set for uniqueness checks\nexisting_questions = set(df[\"Q2\"].str.strip())\n\n# For GPT-2 training, combine topic and question into a single string\ntrain_texts = []\nfor _, row in df.iterrows():\n    topic_str = str(row[\"Q2_Topic\"]).strip()\n    question_str = str(row[\"Q2\"]).strip()\n    combined = f\"Topic: {topic_str}\\nQuestion: {question_str}\\n<|endoftext|>\"\n    train_texts.append(combined)\n\n#############################\n# 2) Custom Dataset\n#############################\nclass TopicQuestionDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=128):\n        self.encodings = []\n        for txt in texts:\n            enc = tokenizer(\n                txt,\n                truncation=True,\n                max_length=max_length,\n                padding=\"max_length\"\n            )\n            self.encodings.append(enc)\n\n    def __len__(self):\n        return len(self.encodings)\n\n    def __getitem__(self, idx):\n        item = self.encodings[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(item[\"attention_mask\"])\n        }\n\n#############################\n# 3) Setup GPT-2\n#############################\nmodel_name = \"gpt2\"  # or \"distilgpt2\" for smaller\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# GPT-2 pad fix\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\n#############################\n# 4) Create Dataset & Collator\n#############################\ntrain_dataset = TopicQuestionDataset(train_texts, tokenizer, max_length=250)\n\ndef data_collator(batch):\n    input_ids = torch.stack([f[\"input_ids\"] for f in batch])\n    attention_mask = torch.stack([f[\"attention_mask\"] for f in batch])\n    labels = input_ids.clone()  # causal language modeling\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n#############################\n# 5) TrainingArguments & Trainer\n#############################\ntraining_args = TrainingArguments(\n    output_dir=\"./temp-output\",\n    overwrite_output_dir=True,\n    num_train_epochs=20,\n    per_device_train_batch_size=2,\n    logging_steps=5,\n    logging_strategy=\"steps\",\n    save_strategy=\"no\",   # no checkpoint saving\n    report_to=[]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset\n)\n\n#############################\n# 6) Fine-Tune GPT-2\n#############################\ntrainer.train()\nprint(\"Training completed.\")\n\n#############################\n# 7) Fuzzy Matching Helper\n#############################\ndef is_too_similar(new_q, existing_set, threshold=0.8):\n    \"\"\"\n    Returns True if 'new_q' is >= threshold similarity\n    with any question in 'existing_set'.\n    Using difflib.SequenceMatcher ratio.\n    \"\"\"\n    for q in existing_set:\n        ratio = difflib.SequenceMatcher(None, new_q, q).ratio()\n        if ratio >= threshold:\n            return True\n    return False\n\n#############################\n# 8) Generate Unique Question\n#############################\ndef generate_unique_question_for_topic(\n    topic,\n    tokenizer=tokenizer,\n    model=model,\n    existing_set=existing_questions,\n    max_length=250,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.5,\n    max_tries=5\n):\n    \"\"\"\n    Generates a new question for the given 'topic' by prompting GPT-2 with:\n    \"Topic: {topic}\\nQuestion:\"\n    Skips if the question is exactly or too similar to existing dataset questions.\n    Tries up to 'max_tries'.\n    \"\"\"\n    for attempt in range(max_tries):\n        # 1) Prepare prompt\n        prompt = f\"Topic: {topic}\\nQuestion:\"\n        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n        input_ids = input_ids.to(model.device)\n\n        # 2) Generate\n        output = model.generate(\n            input_ids=input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            num_return_sequences=1\n        )\n\n        text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # 3) Extract the question portion\n        if \"Question:\" in text:\n            splitted = text.split(\"Question:\")\n            gen_question = splitted[-1].strip()\n        else:\n            gen_question = text.strip()\n\n        # # 4) Check for duplicates or near-duplicates\n        # if gen_question in existing_set:\n        #     print(\"Regenerating (exact match in dataset)...\")\n        #     continue\n        # if is_too_similar(gen_question, existing_set, threshold=fuzzy_threshold):\n        #     print(\"Regenerating (too similar to dataset)...\")\n        #     continue\n\n        # If we get here, it's a new question\n        return gen_question\n\n    return \"No unique question found after multiple attempts.\"\n\n#############################\n# 9) Example Usage\n#############################\nnew_question = generate_unique_question_for_topic(\n    topic=predicted_topic,\n    max_length=250,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.4,\n    max_tries=5\n)\nprint(f\"For topic: '{predicted_topic}'\\nNew question:\\n{new_question}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T20:46:36.394748Z","iopub.execute_input":"2025-03-09T20:46:36.395120Z","iopub.status.idle":"2025-03-09T20:47:20.055672Z","shell.execute_reply.started":"2025-03-09T20:46:36.395097Z","shell.execute_reply":"2025-03-09T20:47:20.054876Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement difflib2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for difflib2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 00:40, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>4.329800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.087900</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.834800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.682300</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.565100</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.437600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.373500</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.329800</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.272200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.254400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.220800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.193000</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.195500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.165700</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.152400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.116100</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.155000</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.131100</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.108800</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.104900</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.110200</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.111900</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.079500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.104400</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.096300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.083800</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.098000</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.081700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Training completed.\nFor topic: '['the account of the compilation of the Qurâ€™an under the Rightly Guided Caliphs ']'\nNew question:\n(a) Write about the different ways in which the Qurâ€™an was compiled after the Prophetâ€™s death. (b) How can Muslims today use the Qurâ€™an to develop a closer connection with God?\n","output_type":"stream"}],"execution_count":15}]}