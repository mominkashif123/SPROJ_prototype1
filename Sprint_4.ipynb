{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11287747,"sourceType":"datasetVersion","datasetId":7057711}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers==4.31.0\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:01:50.741703Z","iopub.execute_input":"2025-04-05T18:01:50.741926Z","iopub.status.idle":"2025-04-05T18:02:03.709809Z","shell.execute_reply.started":"2025-04-05T18:01:50.741900Z","shell.execute_reply":"2025-04-05T18:02:03.708861Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.11 requires transformers>=4.33.1, but you have transformers 4.31.0 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.41.1 transformers==4.31.0 trl==0.4.7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:03.711020Z","iopub.execute_input":"2025-04-05T18:02:03.711307Z","iopub.status.idle":"2025-04-05T18:02:11.646276Z","shell.execute_reply.started":"2025-04-05T18:02:03.711278Z","shell.execute_reply":"2025-04-05T18:02:11.645192Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -U bitsandbytes accelerate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:11.648188Z","iopub.execute_input":"2025-04-05T18:02:11.648543Z","iopub.status.idle":"2025-04-05T18:02:18.487313Z","shell.execute_reply.started":"2025-04-05T18:02:11.648512Z","shell.execute_reply":"2025-04-05T18:02:18.486512Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.1)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\nCollecting accelerate\n  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.4-py3-none-manylinux_2_24_x86_64.whl (76.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate\n  Attempting uninstall: bitsandbytes\n    Found existing installation: bitsandbytes 0.41.1\n    Uninstalling bitsandbytes-0.41.1:\n      Successfully uninstalled bitsandbytes-0.41.1\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.21.0\n    Uninstalling accelerate-0.21.0:\n      Successfully uninstalled accelerate-0.21.0\nSuccessfully installed accelerate-1.6.0 bitsandbytes-0.45.4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import bitsandbytes as bnb\nimport accelerate\nimport transformers\nimport trl\nimport peft\n\nprint(\"bitsandbytes:\", bnb.__version__)\nprint(\"accelerate:\", accelerate.__version__)\nprint(\"transformers:\", transformers.__version__)\nprint(\"trl:\", trl.__version__)\nprint(\"peft:\", peft.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:18.489031Z","iopub.execute_input":"2025-04-05T18:02:18.489370Z","iopub.status.idle":"2025-04-05T18:02:37.380886Z","shell.execute_reply.started":"2025-04-05T18:02:18.489337Z","shell.execute_reply":"2025-04-05T18:02:37.379951Z"}},"outputs":[{"name":"stdout","text":"bitsandbytes: 0.45.4\naccelerate: 1.6.0\ntransformers: 4.31.0\ntrl: 0.4.7\npeft: 0.4.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:37.381864Z","iopub.execute_input":"2025-04-05T18:02:37.382541Z","iopub.status.idle":"2025-04-05T18:02:37.387616Z","shell.execute_reply.started":"2025-04-05T18:02:37.382506Z","shell.execute_reply":"2025-04-05T18:02:37.386802Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"NousResearch/Llama-2-7b-hf\"\n\n# Fine-tuned model name\nnew_model = \"Llama-2-7b-chat-finetune\"\n\n################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs (increase from 1 to 5)\nnum_train_epochs = 10\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:37.388406Z","iopub.execute_input":"2025-04-05T18:02:37.388711Z","iopub.status.idle":"2025-04-05T18:02:37.486052Z","shell.execute_reply.started":"2025-04-05T18:02:37.388675Z","shell.execute_reply":"2025-04-05T18:02:37.485049Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\n\n# Load your CSV\ndf = pd.read_csv(\"/kaggle/input/dataset11/Q2_dataset_fazul.csv\")\n\n# Convert non-string columns to string before concatenation\ndf[\"Year\"] = df[\"Year\"].astype(str)\ndf[\"Paper_Varient\"] = df[\"Paper_Varient\"].astype(str)\n\n# Create a single text column with all the info\ndf[\"text\"] = (\n    \"Year: \" + df[\"Year\"] + \"\\n\" +\n    \"Paper_Session: \" + df[\"Paper_Session\"] + \"\\n\" +\n    \"Paper_Varient: \" + df[\"Paper_Varient\"] + \"\\n\" +\n    \"Q2_Topic: \" + df[\"Q2_Topic\"] + \"\\n\" +\n    \"Q2: \" + df[\"Q2\"]\n)\n\n# Now df[\"text\"] holds your combined text\nprint(df[\"text\"].iloc[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:37.486804Z","iopub.execute_input":"2025-04-05T18:02:37.487054Z","iopub.status.idle":"2025-04-05T18:02:37.520400Z","shell.execute_reply.started":"2025-04-05T18:02:37.487033Z","shell.execute_reply":"2025-04-05T18:02:37.519780Z"}},"outputs":[{"name":"stdout","text":"Year: 2023\nPaper_Session: ON\nPaper_Varient: 11\nQ2_Topic: the major themes of the Qur’an as contained both in the passages set for special study and in other similar passages\nQ2: (a) Using the set passages you have studied, write about God’s relationship with the created world.  (b) What do these passages teach Muslims about responsibility towards their environment?\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\n\n# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n\n\n# Inspect the first combined text\nprint(\"SAMPLE TEXT:\\n\", df[\"text\"].iloc[0])\ntexts = df[\"text\"].tolist()\n\n\n# -----------------------\n# 2. Tokenize\n# -----------------------\n\n# We'll create a simple custom dataset class that returns tokenized input.\nclass Q2Dataset(torch.utils.data.Dataset):\n    def __init__(self, texts, tokenizer, max_length=512):\n        self.examples = []\n        for t in texts:\n            # Tokenize each text with truncation\n            tokenized = tokenizer(\n                t,\n                truncation=True,\n                max_length=max_length,\n                return_tensors=\"pt\"\n            )\n            self.examples.append({\n                \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n                \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n            })\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        return self.examples[idx]  # ✅ Properly indented\n\n\ndataset = Q2Dataset(texts, tokenizer, max_length=512)\n\n# You could split into train/test if you want. For small data, we’ll skip that:\ntrain_dataset = dataset\n\nprint(\"Number of training samples:\", len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:02:37.522516Z","iopub.execute_input":"2025-04-05T18:02:37.522762Z","iopub.status.idle":"2025-04-05T18:03:50.304309Z","shell.execute_reply.started":"2025-04-05T18:02:37.522740Z","shell.execute_reply":"2025-04-05T18:03:50.303478Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc622f16cf434c4494a15e204e4557ae"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334a8af9107e4092a21e70437739c41c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59eaeff23fe745d5a9a8db0296a95bb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3863eb053e441aa97c61baffe79957a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9268fcb772ae49c984d5921f09dad02b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45367145139449919e99e2982c2928af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23c5060b855459593ceb41e9553e2bf"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"803f56d920de43b6a4594d7e86776933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a70ec14a978b4043b3fca58323417cbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"758e2ab01d8d4e60919f5c261b72016c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8a8837f5d7428d99f3c4bcf9b90cb5"}},"metadata":{}},{"name":"stdout","text":"SAMPLE TEXT:\n Year: 2023\nPaper_Session: ON\nPaper_Varient: 11\nQ2_Topic: the major themes of the Qur’an as contained both in the passages set for special study and in other similar passages\nQ2: (a) Using the set passages you have studied, write about God’s relationship with the created world.  (b) What do these passages teach Muslims about responsibility towards their environment?\nNumber of training samples: 28\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n\n# Train model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:03:50.305613Z","iopub.execute_input":"2025-04-05T18:03:50.306162Z","iopub.status.idle":"2025-04-05T18:13:09.921612Z","shell.execute_reply.started":"2025-04-05T18:03:50.306137Z","shell.execute_reply":"2025-04-05T18:13:09.920908Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [70/70 08:59, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.681900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.632000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=70, training_loss=0.9512085778372629, metrics={'train_runtime': 547.3007, 'train_samples_per_second': 0.512, 'train_steps_per_second': 0.128, 'total_flos': 778065218273280.0, 'train_loss': 0.9512085778372629, 'epoch': 10.0})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:14:34.349351Z","iopub.execute_input":"2025-04-05T18:14:34.349687Z","iopub.status.idle":"2025-04-05T18:14:34.579553Z","shell.execute_reply.started":"2025-04-05T18:14:34.349665Z","shell.execute_reply":"2025-04-05T18:14:34.578843Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"prompt = \"\"\"\nYou are an O-level exam question generator that will predict new question based on pattern depicted in datatset.\nPlease output the question in the following format:\n(a)\n(b)\n\nOnly output the question text, nothing else.\n\n(a) Describe how the Qur’an addresses the concept of God’s oneness.\n(b) Explain why understanding this concept is essential for a Muslim’s faith.\n\nNow create a **new** question in the same (a)/(b) format, focusing on the major themes of the Qur’an:\n(a)\n(b)\n\"\"\"\n\n# Use the LoRA-updated model\npeft_model = trainer.model\npeft_model.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(peft_model.device)\nwith torch.no_grad():\n    output_ids = peft_model.generate(\n        **inputs,\n        max_length=300,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True\n    )\n\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\nprint(\"Generated question:\")\nprint(generated_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-05T18:22:03.837149Z","iopub.execute_input":"2025-04-05T18:22:03.837531Z","iopub.status.idle":"2025-04-05T18:24:49.869542Z","shell.execute_reply.started":"2025-04-05T18:22:03.837501Z","shell.execute_reply":"2025-04-05T18:24:49.868565Z"}},"outputs":[{"name":"stdout","text":"Generated question:\n\nYou are an O-level exam question generator that will predict new question based on pattern depicted in datatset.\nPlease output the question in the following format:\n(a)\n(b)\n\nOnly output the question text, nothing else.\n\n(a) Describe how the Qur’an addresses the concept of God’s oneness.\n(b) Explain why understanding this concept is essential for a Muslim’s faith.\n\nNow create a **new** question in the same (a)/(b) format, focusing on the major themes of the Qur’an:\n(a)\n(b)\n\nOnly output the question text, nothing else.\n\n(a) The Qur’an describes God’s creation of the heavens and the earth. Why do you think it is important for Muslims to know about this?\n(b) How do Muslims use this knowledge to understand God’s power and greatness?\n\n### Exercise:\n\n(a) Write your own question in the format above.\n(b) Explain why the Qur’an is important for Muslims to study and understand.\n\n\n\n\n\n\n","output_type":"stream"}],"execution_count":14}]}