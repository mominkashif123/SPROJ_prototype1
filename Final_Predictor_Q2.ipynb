{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10971755,"sourceType":"datasetVersion","datasetId":6827064},{"sourceId":10971920,"sourceType":"datasetVersion","datasetId":6827185},{"sourceId":10972326,"sourceType":"datasetVersion","datasetId":6827489},{"sourceId":10972766,"sourceType":"datasetVersion","datasetId":6827809}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.compose import ColumnTransformer\n\n# 1. Load Data\ndf = pd.read_csv(\"/kaggle/input/q2-pp2/Q2_pp2_dataset_complete.csv\")\n\n# Identify and remove rare classes (classes with only one sample)\nclass_counts = df['Q2_Topics'].value_counts()\nrare_classes = class_counts[class_counts == 1].index\ndf = df[~df['Q2_Topics'].isin(rare_classes)]\n\n# 2. Feature Engineering\nlabel_encoder = LabelEncoder()\ndf['Q2_Topics'] = df['Q2_Topics'].astype(str)\ndf['Q2_Topic_Encoded'] = label_encoder.fit_transform(df['Q2_Topics'])\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(handle_unknown='ignore'), ['Paper_Session'])],\n    remainder='passthrough'\n)\n\nX = df[['Year', 'Paper_Session', 'Paper_Varient']]\nX.columns = X.columns.astype(str)\nX = ct.fit_transform(X)\nX = pd.DataFrame(X)\nX.columns = X.columns.astype(str)\ny = df['Q2_Topic_Encoded']\n\n# 3. Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# 4. Model Training\nmodels = {\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n}\n\nfor model_name, model in models.items():\n    print(f\"Training {model_name}...\")\n    model.fit(X_train, y_train)\n\n    # 5. Evaluation\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy ({model_name}): {accuracy}\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # 6. Prediction for Next Year\n    next_year_data = pd.DataFrame({\n        'Year': [2024],\n        'Paper_Session': ['MJ'],\n        'Paper_Varient': ['22']\n    })\n\n    X_next_year = next_year_data[['Year', 'Paper_Session', 'Paper_Varient']]\n    X_next_year.columns = X_next_year.columns.astype(str)\n    X_next_year = ct.transform(X_next_year)\n    X_next_year = pd.DataFrame(X_next_year)\n    X_next_year.columns = X_next_year.columns.astype(str)\n\n    predicted_topic_encoded = model.predict(X_next_year)\n    predicted_topic = label_encoder.inverse_transform(predicted_topic_encoded)\n    print(f\"Predicted Topic for Next Year ({model_name}): {predicted_topic}\")\n    print(\"-\" * 50)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T18:11:43.497388Z","iopub.execute_input":"2025-03-09T18:11:43.497816Z","iopub.status.idle":"2025-03-09T18:11:45.757177Z","shell.execute_reply.started":"2025-03-09T18:11:43.497765Z","shell.execute_reply":"2025-03-09T18:11:45.755920Z"}},"outputs":[{"name":"stdout","text":"Training Random Forest...\nAccuracy (Random Forest): 0.0\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00       1.0\n           1       0.00      0.00      0.00       1.0\n           2       0.00      0.00      0.00       1.0\n\n    accuracy                           0.00       3.0\n   macro avg       0.00      0.00      0.00       3.0\nweighted avg       0.00      0.00      0.00       3.0\n\nPredicted Topic for Next Year (Random Forest): ['their use in legal thinking, and their relationship with the Qur’an, consensus (ijma‘) and analogy (qiyas)']\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.compose import ColumnTransformer\nimport string\n\n# 1. Load and Preprocess Data\ndf = pd.read_csv(\"/kaggle/input/2013-dataset/Q2_dataset_complete2.csv\")\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Add more cleaning steps if needed (e.g., stemming, stop word removal)\n    return text\n\ndf['Q2'] = df['Q2'].apply(preprocess_text)\n\n# 2. Feature Engineering\nlabel_encoder = LabelEncoder()\ndf['Q2_Topic'] = df['Q2_Topic'].astype(str)\ndf['Q2_Topic_Encoded'] = label_encoder.fit_transform(df['Q2_Topic'])\n\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Include bigrams\ntfidf_features = tfidf_vectorizer.fit_transform(df['Q2'])\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(handle_unknown='ignore'), ['Paper_Session'])],\n    remainder='passthrough'\n)\n\nX = pd.concat([pd.DataFrame(tfidf_features.toarray()), df[['Year', 'Paper_Session', 'Paper_Varient']]], axis=1)\nX.columns = X.columns.astype(str)\nX = ct.fit_transform(X)\nX = pd.DataFrame(X)\nX.columns = X.columns.astype(str)\ny = df['Q2_Topic_Encoded']\n\n# 3. Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# 4. Model Training (Random Forest with Hyperparameter Tuning)\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [2, 5, 10]\n}\n\nmodel = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\nbest_model = grid_search.best_estimator_\n\n# 5. Evaluation\ny_pred = best_model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# 6. Prediction for Next Year\nnext_year_data = pd.DataFrame({\n    'Year': [2024],\n    'Paper_Session': ['ON'],\n    'Paper_Varient': ['11'],\n    'Q2': [\"\"]\n})\n\nnext_year_data['Q2'] = next_year_data['Q2'].apply(preprocess_text)\nnext_year_tfidf = tfidf_vectorizer.transform(next_year_data['Q2'])\n\nX_next_year = pd.concat([pd.DataFrame(next_year_tfidf.toarray()), next_year_data[['Year', 'Paper_Session', 'Paper_Varient']]], axis=1)\nX_next_year.columns = X_next_year.columns.astype(str)\nX_next_year = ct.transform(X_next_year)\nX_next_year = pd.DataFrame(X_next_year)\nX_next_year.columns = X_next_year.columns.astype(str)\n\npredicted_topic_encoded = best_model.predict(X_next_year)\npredicted_topic = label_encoder.inverse_transform(predicted_topic_encoded)\nprint(f\"Predicted Topic for Next Year: {predicted_topic}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T17:13:37.211430Z","iopub.execute_input":"2025-03-09T17:13:37.211850Z","iopub.status.idle":"2025-03-09T17:13:37.451026Z","shell.execute_reply.started":"2025-03-09T17:13:37.211821Z","shell.execute_reply":"2025-03-09T17:13:37.449340Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Q2'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-424aba86ab17>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Q2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# 2. Feature Engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Q2'"],"ename":"KeyError","evalue":"'Q2'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"!pip install imbalanced-learn xgboost\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\n\n# 1) Load & Prep\ndf = pd.read_csv(\"/kaggle/input/2013-dataset/Q2_dataset_complete2.csv\")\n\n# remove classes w only 1 sample\nclass_counts = df['Q2_Topics'].value_counts()\nrare_classes = class_counts[class_counts == 1].index\ndf = df[~df['Q2_Topics'].isin(rare_classes)]\n\n# encode target\nlabel_encoder = LabelEncoder()\ndf['Q2_Topics'] = df['Q2_Topics'].astype(str)\ndf['Q2_Topic_Encoded'] = label_encoder.fit_transform(df['Q2_Topics'])\n\n# separate features/target\nnumeric_features = ['Year', 'Paper_Varient']\ncategorical_features = ['Paper_Session']\n\nX = df[numeric_features + categorical_features]\ny = df['Q2_Topic_Encoded']\n\n# column transform (OneHot + Scale numeric)\npreprocessor = ColumnTransformer([\n    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n    ('num', StandardScaler(), numeric_features)\n])\n\nX_trans = preprocessor.fit_transform(X)\n\n# 2) SMOTE once on entire dataset\nsm = SMOTE(random_state=42, k_neighbors=1)  # if smallest class has at least 2 samples\nX_smote, y_smote = sm.fit_resample(X_trans, y)\n\nprint(\"After SMOTE:\", X_smote.shape, y_smote.shape)\n\n# 3) Train-test or CV on X_smote, y_smote\nmodels = {\n    \"XGB\": XGBClassifier(random_state=42),\n    \"MLP\": MLPClassifier(max_iter=500, random_state=42),\n    \"GB\": GradientBoostingClassifier(random_state=42),\n    \"SVM\": SVC(random_state=42),\n    \"RF\": RandomForestClassifier(random_state=42),\n}\n\nparam_grids = {\n    \"XGB\": {\n        'n_estimators': [50, 100],\n        'max_depth': [3, 5],\n        'learning_rate': [0.01, 0.1,0.001]\n    },\n    \"MLP\": {\n        'hidden_layer_sizes': [(100,), (50,50)],\n        'activation': ['relu', 'tanh'],\n        'alpha': [0.001, 0.01,0.0001]\n    },\n    \"GB\": {\n        'n_estimators': [50, 100],\n        'learning_rate': [0.01, 0.1,0.001],\n        'max_depth': [3, 5]\n    },\n    \"SVM\": {\n        'C': [1, 10],\n        'kernel': ['linear', 'rbf']\n    },\n    \"RF\": {\n        'n_estimators': [50, 100],\n        'max_depth': [3, 5, None]\n    }\n}\n\ncv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # or 5\n\nfor name, model in models.items():\n    print(f\"\\n--- {name} ---\")\n    if name in param_grids:\n        grid = GridSearchCV(\n            model,\n            param_grids[name],\n            cv=cv,\n            scoring='accuracy',\n            n_jobs=-1\n        )\n        grid.fit(X_smote, y_smote)\n        best_model = grid.best_estimator_\n        print(\"Best Params:\", grid.best_params_)\n        print(\"CV Best Score:\", grid.best_score_)\n    else:\n        model.fit(X_smote, y_smote)\n        best_model = model\n    \n    # Evaluate holdout if you want\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_smote, y_smote, test_size=0.3, random_state=42, stratify=y_smote\n    )\n    best_model.fit(X_train, y_train)\n    y_pred = best_model.predict(X_test)\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"Holdout Accuracy: {acc:.3f}\")\n    print(classification_report(y_test, y_pred, zero_division=0))\n\n    # Next year\n    next_data = pd.DataFrame({'Year':[2025], 'Paper_Varient':[12], 'Paper_Session':['MJ']})\n    next_data_trans = preprocessor.transform(next_data)\n    pred_label = best_model.predict(next_data_trans)\n    print(\"Predicted Topic:\", label_encoder.inverse_transform(pred_label))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T17:14:47.409556Z","iopub.execute_input":"2025-03-09T17:14:47.409988Z","iopub.status.idle":"2025-03-09T17:15:05.215143Z","shell.execute_reply.started":"2025-03-09T17:14:47.409953Z","shell.execute_reply":"2025-03-09T17:15:05.213787Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\nRequirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->imbalanced-learn) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->imbalanced-learn) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->imbalanced-learn) (2024.2.0)\nAfter SMOTE: (20, 3) (20,)\n\n--- XGB ---\nBest Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\nCV Best Score: 0.3968253968253968\nHoldout Accuracy: 0.167\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         2\n           1       0.00      0.00      0.00         1\n           2       0.00      0.00      0.00         2\n           3       0.20      1.00      0.33         1\n\n    accuracy                           0.17         6\n   macro avg       0.05      0.25      0.08         6\nweighted avg       0.03      0.17      0.06         6\n\nPredicted Topic: ['the revelation of the Qur’an to the Prophet (pbuh) between the years 610 and 632']\n\n--- MLP ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Best Params: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}\nCV Best Score: 0.3968253968253968\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Holdout Accuracy: 0.333\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         2\n           1       0.20      1.00      0.33         1\n           2       0.00      0.00      0.00         2\n           3       1.00      1.00      1.00         1\n\n    accuracy                           0.33         6\n   macro avg       0.30      0.50      0.33         6\nweighted avg       0.20      0.33      0.22         6\n\nPredicted Topic: ['the revelation of the Qur’an to the Prophet (pbuh) between the years 610 and 632']\n\n--- GB ---\nBest Params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\nCV Best Score: 0.5\nHoldout Accuracy: 0.333\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         2\n           1       0.20      1.00      0.33         1\n           2       0.00      0.00      0.00         2\n           3       1.00      1.00      1.00         1\n\n    accuracy                           0.33         6\n   macro avg       0.30      0.50      0.33         6\nweighted avg       0.20      0.33      0.22         6\n\nPredicted Topic: ['the revelation of the Qur’an to the Prophet (pbuh) between the years 610 and 632']\n\n--- SVM ---\nBest Params: {'C': 10, 'kernel': 'rbf'}\nCV Best Score: 0.3968253968253968\nHoldout Accuracy: 0.333\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         2\n           1       0.25      1.00      0.40         1\n           2       0.00      0.00      0.00         2\n           3       0.50      1.00      0.67         1\n\n    accuracy                           0.33         6\n   macro avg       0.19      0.50      0.27         6\nweighted avg       0.12      0.33      0.18         6\n\nPredicted Topic: ['the revelation of the Qur’an to the Prophet (pbuh) between the years 610 and 632']\n\n--- RF ---\nBest Params: {'max_depth': 3, 'n_estimators': 50}\nCV Best Score: 0.5\nHoldout Accuracy: 0.333\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         2\n           1       0.20      1.00      0.33         1\n           2       0.00      0.00      0.00         2\n           3       1.00      1.00      1.00         1\n\n    accuracy                           0.33         6\n   macro avg       0.30      0.50      0.33         6\nweighted avg       0.20      0.33      0.22         6\n\nPredicted Topic: ['the revelation of the Qur’an to the Prophet (pbuh) between the years 610 and 632']\n","output_type":"stream"}],"execution_count":10}]}