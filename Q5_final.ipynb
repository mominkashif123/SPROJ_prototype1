{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11391578,"sourceType":"datasetVersion","datasetId":7134075},{"sourceId":11392137,"sourceType":"datasetVersion","datasetId":7134494}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport joblib  # for saving/loading models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.compose import ColumnTransformer\n\n# 1. Load Data\ndf = pd.read_csv(\"/kaggle/input/paper1-dataset/Q5_prediction_complete.csv\")\n\n# Identify and remove rare classes (classes with only one sample)\nclass_counts = df['Q5_Topic'].value_counts()\nrare_classes = class_counts[class_counts == 1].index\ndf = df[~df['Q5_Topic'].isin(rare_classes)]\n\n# 2. Feature Engineering\nlabel_encoder = LabelEncoder()\ndf['Q5_Topic'] = df['Q5_Topic'].astype(str)\ndf['Q5_Topic_Encoded'] = label_encoder.fit_transform(df['Q5_Topic'])\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(handle_unknown='ignore'), ['Paper_Session'])],\n    remainder='passthrough'\n)\n\nX = df[['Year', 'Paper_Session', 'Paper_Varient']]\nX.columns = X.columns.astype(str)\nX_transformed = ct.fit_transform(X)  # fit_transform for training data\ny = df['Q5_Topic_Encoded']\n\n# 3. Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 4. Model Training\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Evaluation\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# 6. Prediction for Next Year (example)\nnext_year_data = pd.DataFrame({\n    'Year': [2025],\n    'Paper_Session': ['MJ'],\n    'Paper_Varient': ['12']\n})\n# We must transform it with the same ColumnTransformer\nX_next_year = ct.transform(next_year_data)\npredicted_topic_encoded = model.predict(X_next_year)\npredicted_topic = label_encoder.inverse_transform(predicted_topic_encoded)\nprint(f\"Predicted Topic for 2025: {predicted_topic[0]}\")\n\n# 7. SAVE EVERYTHING\n# We'll save: the trained model, the column transformer, and the label encoder\njoblib.dump(model, \"rf_model.pkl\")            # saves the RandomForest\njoblib.dump(ct, \"column_transformer.pkl\")     # saves the ColumnTransformer\njoblib.dump(label_encoder, \"label_encoder.pkl\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T19:39:34.054612Z","iopub.execute_input":"2025-04-13T19:39:34.054951Z","iopub.status.idle":"2025-04-13T19:39:35.601535Z","shell.execute_reply.started":"2025-04-13T19:39:34.054926Z","shell.execute_reply":"2025-04-13T19:39:35.600502Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.133\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         2\n           1       0.00      0.00      0.00         1\n           2       0.33      0.33      0.33         3\n           3       0.00      0.00      0.00         3\n           4       0.00      0.00      0.00         1\n           5       0.00      0.00      0.00         1\n           6       0.33      0.25      0.29         4\n\n    accuracy                           0.13        15\n   macro avg       0.10      0.08      0.09        15\nweighted avg       0.16      0.13      0.14        15\n\nPredicted Topic for 2025: his leading Companions, including the Ten Blessed Companions during his lifetime.\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['label_encoder.pkl']"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets imbalanced-learn difflib2 --quiet\n\nimport pandas as pd\nimport torch\nimport difflib\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments\n)\n\n#############################\n# 1) Load & Combine Dataset\n#############################\ndf = pd.read_csv(\"/kaggle/input/generation-dataset/Q5_dataset_complete.csv\")\ndf = df.dropna(subset=[\"Q5_Topic\", \"Q5\"])  # remove any empty rows\n\n# We'll store all existing questions in a set for uniqueness checks\nexisting_questions = set(df[\"Q5\"].str.strip())\n\n# For GPT-2 training, combine topic and question into a single string\ntrain_texts = []\nfor _, row in df.iterrows():\n    topic_str = str(row[\"Q5_Topic\"]).strip()\n    question_str = str(row[\"Q5\"]).strip()\n    combined = f\"Topic: {topic_str}\\nQuestion: {question_str}\\n<|endoftext|>\"\n    train_texts.append(combined)\n\n#############################\n# 2) Custom Dataset\n#############################\nclass TopicQuestionDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=128):\n        self.encodings = []\n        for txt in texts:\n            enc = tokenizer(\n                txt,\n                truncation=True,\n                max_length=max_length,\n                padding=\"max_length\"\n            )\n            self.encodings.append(enc)\n\n    def __len__(self):\n        return len(self.encodings)\n\n    def __getitem__(self, idx):\n        item = self.encodings[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(item[\"attention_mask\"])\n        }\n\n\nmodel_name = \"gpt2\"  # or \"distilgpt2\" for smaller\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# GPT-2 pad fix\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\n\ntrain_dataset = TopicQuestionDataset(train_texts, tokenizer, max_length=250)\n\ndef data_collator(batch):\n    input_ids = torch.stack([f[\"input_ids\"] for f in batch])\n    attention_mask = torch.stack([f[\"attention_mask\"] for f in batch])\n    labels = input_ids.clone()  # causal language modeling\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./temp-output\",\n    overwrite_output_dir=True,\n    num_train_epochs=20,\n    per_device_train_batch_size=2,\n    logging_steps=5,\n    logging_strategy=\"steps\",\n    save_strategy=\"no\",   # no checkpoint saving\n    report_to=[]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset\n)\n\n#############################\n# 6) Fine-Tune GPT-2\n#############################\ntrainer.train()\nprint(\"Training completed.\")\n\n\n# def is_too_similar(new_q, existing_set, threshold=0.8):\n#     \"\"\"\n#     Returns True if 'new_q' is >= threshold similarity\n#     with any question in 'existing_set'.\n#     Using difflib.SequenceMatcher ratio.\n#     \"\"\"\n#     for q in existing_set:\n#         ratio = difflib.SequenceMatcher(None, new_q, q).ratio()\n#         if ratio >= threshold:\n#             return True\n#     return False\n\ndef generate_unique_question_for_topic(\n    topic,\n    tokenizer=tokenizer,\n    model=model,\n    existing_set=existing_questions,\n    max_length=300,\n    temperature=0.5,\n    top_p=0.9,\n    fuzzy_threshold=0.5,\n    max_tries=5\n):\n    \"\"\"\n    Generates a new question for the given 'topic' by prompting GPT-2 with:\n    \"Topic: {topic}\\nQuestion:\"\n    Skips if the question is exactly or too similar to existing dataset questions.\n    Tries up to 'max_tries'.\n    \"\"\"\n    for attempt in range(max_tries):\n        # 1) Prepare prompt\n        prompt = f\"Topic: {topic}\\nQuestion:\"\n        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n        input_ids = input_ids.to(model.device)\n\n        # 2) Generate\n        output = model.generate(\n            input_ids=input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            num_return_sequences=1\n        )\n\n        text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # 3) Extract the question portion\n        if \"Question:\" in text:\n            splitted = text.split(\"Question:\")\n            gen_question = splitted[-1].strip()\n        else:\n            gen_question = text.strip()\n\n    \n        return gen_question\n\n    return \"No unique question found after multiple attempts.\"\n\n\nnew_question = generate_unique_question_for_topic(\n    topic=predicted_topic,\n    max_length=300,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.4,\n    max_tries=5\n)\nprint(f\"For topic: '{predicted_topic}'\\nNew question:\\n{new_question}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T19:39:35.602938Z","iopub.execute_input":"2025-04-13T19:39:35.603287Z","iopub.status.idle":"2025-04-13T19:41:28.359558Z","shell.execute_reply.started":"2025-04-13T19:39:35.603254Z","shell.execute_reply":"2025-04-13T19:41:28.358369Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement difflib2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for difflib2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7e4ed9b083b46d585971d3d73864718"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f909488452541d3a80d513053a61fb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63909833c78e413197d2a9b0199e5f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e26f1f458dc94007afe599b1224f2c3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"958816b5601e4e54b419578edc33419d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dbd638123194565912bf015468f1ace"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc80d1a279043ce871e4f2bae28e9df"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='280' max='280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [280/280 01:16, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>4.443800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.097100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.845200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.742000</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.574000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.514100</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.422200</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.445000</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.329600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.306300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.394200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.318400</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.309000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.261400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.237300</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.280200</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.230900</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.202500</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.209600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.236200</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.190800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.198300</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.172800</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.190600</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.168300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.152200</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.174800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.152000</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.145500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.151900</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.137000</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.136300</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.119100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.136400</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.104900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.136700</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.114800</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.098900</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.133300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.100100</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.111500</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.120000</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.103500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.090700</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.102700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.107900</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.090000</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.106200</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.095600</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.088800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.093900</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.091200</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.098100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.093400</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.091700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.089600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Training completed.\nFor topic: '['his leading Companions, including the Ten Blessed Companions during his lifetime.']'\nNew question:\n(a) Write about the lives of the Companions Khalid ibn al-Khattab and Abu Sufyan ibn Harb. [10] (b) How can Muslims follow the example of the Prophetâ€™s wives?\n","output_type":"stream"}],"execution_count":2}]}