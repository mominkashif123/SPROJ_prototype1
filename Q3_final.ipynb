{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11391578,"sourceType":"datasetVersion","datasetId":7134075},{"sourceId":11392137,"sourceType":"datasetVersion","datasetId":7134494}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport joblib  # for saving/loading models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.compose import ColumnTransformer\n\n# 1. Load Data\ndf = pd.read_csv(\"/kaggle/input/paper1-dataset/Q3_prediction_dataset.csv\")\n\n# Identify and remove rare classes (classes with only one sample)\nclass_counts = df['Q3_Topic'].value_counts()\nrare_classes = class_counts[class_counts == 1].index\ndf = df[~df['Q3_Topic'].isin(rare_classes)]\n\n# 2. Feature Engineering\nlabel_encoder = LabelEncoder()\ndf['Q3_Topic'] = df['Q3_Topic'].astype(str)\ndf['Q3_Topic_Encoded'] = label_encoder.fit_transform(df['Q3_Topic'])\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(handle_unknown='ignore'), ['Paper_Session'])],\n    remainder='passthrough'\n)\n\nX = df[['Year', 'Paper_Session', 'Paper_Varient']]\nX.columns = X.columns.astype(str)\nX_transformed = ct.fit_transform(X)  # fit_transform for training data\ny = df['Q3_Topic_Encoded']\n\n# 3. Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 4. Model Training\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Evaluation\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# 6. Prediction for Next Year (example)\nnext_year_data = pd.DataFrame({\n    'Year': [2025],\n    'Paper_Session': ['MJ'],\n    'Paper_Varient': ['12']\n})\n# We must transform it with the same ColumnTransformer\nX_next_year = ct.transform(next_year_data)\npredicted_topic_encoded = model.predict(X_next_year)\npredicted_topic = label_encoder.inverse_transform(predicted_topic_encoded)\nprint(f\"Predicted Topic for 2025: {predicted_topic[0]}\")\n\n# 7. SAVE EVERYTHING\n# We'll save: the trained model, the column transformer, and the label encoder\njoblib.dump(model, \"rf_model.pkl\")            # saves the RandomForest\njoblib.dump(ct, \"column_transformer.pkl\")     # saves the ColumnTransformer\njoblib.dump(label_encoder, \"label_encoder.pkl\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:53:28.072176Z","iopub.execute_input":"2025-04-13T18:53:28.072506Z","iopub.status.idle":"2025-04-13T18:53:29.561310Z","shell.execute_reply.started":"2025-04-13T18:53:28.072475Z","shell.execute_reply":"2025-04-13T18:53:29.560498Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.167\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         4\n           1       0.00      0.00      0.00         1\n           2       0.00      0.00      0.00         3\n           3       0.00      0.00      0.00         0\n           4       0.25      0.25      0.25         4\n           5       0.29      0.50      0.36         4\n           6       0.00      0.00      0.00         2\n\n    accuracy                           0.17        18\n   macro avg       0.08      0.11      0.09        18\nweighted avg       0.12      0.17      0.14        18\n\nPredicted Topic for 2025: the importance of his actions as examples for Muslim individuals in their personal conduct and relations with others including women and non-Muslims\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['label_encoder.pkl']"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!pip install transformers datasets imbalanced-learn difflib2 --quiet\n\nimport pandas as pd\nimport torch\nimport difflib\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments\n)\n\n#############################\n# 1) Load & Combine Dataset\n#############################\ndf = pd.read_csv(\"/kaggle/input/generation-dataset/Q3_dataset_complete.csv\")\ndf = df.dropna(subset=[\"Q3_Topic\", \"Q3\"])  # remove any empty rows\n\n# We'll store all existing questions in a set for uniqueness checks\nexisting_questions = set(df[\"Q3\"].str.strip())\n\n# For GPT-2 training, combine topic and question into a single string\ntrain_texts = []\nfor _, row in df.iterrows():\n    topic_str = str(row[\"Q3_Topic\"]).strip()\n    question_str = str(row[\"Q3\"]).strip()\n    combined = f\"Topic: {topic_str}\\nQuestion: {question_str}\\n<|endoftext|>\"\n    train_texts.append(combined)\n\n#############################\n# 2) Custom Dataset\n#############################\nclass TopicQuestionDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=128):\n        self.encodings = []\n        for txt in texts:\n            enc = tokenizer(\n                txt,\n                truncation=True,\n                max_length=max_length,\n                padding=\"max_length\"\n            )\n            self.encodings.append(enc)\n\n    def __len__(self):\n        return len(self.encodings)\n\n    def __getitem__(self, idx):\n        item = self.encodings[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(item[\"attention_mask\"])\n        }\n\n\nmodel_name = \"gpt2\"  # or \"distilgpt2\" for smaller\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# GPT-2 pad fix\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\n\ntrain_dataset = TopicQuestionDataset(train_texts, tokenizer, max_length=250)\n\ndef data_collator(batch):\n    input_ids = torch.stack([f[\"input_ids\"] for f in batch])\n    attention_mask = torch.stack([f[\"attention_mask\"] for f in batch])\n    labels = input_ids.clone()  # causal language modeling\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./temp-output\",\n    overwrite_output_dir=True,\n    num_train_epochs=20,\n    per_device_train_batch_size=2,\n    logging_steps=5,\n    logging_strategy=\"steps\",\n    save_strategy=\"no\",   # no checkpoint saving\n    report_to=[]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset\n)\n\n#############################\n# 6) Fine-Tune GPT-2\n#############################\ntrainer.train()\nprint(\"Training completed.\")\n\n\n# def is_too_similar(new_q, existing_set, threshold=0.8):\n#     \"\"\"\n#     Returns True if 'new_q' is >= threshold similarity\n#     with any question in 'existing_set'.\n#     Using difflib.SequenceMatcher ratio.\n#     \"\"\"\n#     for q in existing_set:\n#         ratio = difflib.SequenceMatcher(None, new_q, q).ratio()\n#         if ratio >= threshold:\n#             return True\n#     return False\n\ndef generate_unique_question_for_topic(\n    topic,\n    tokenizer=tokenizer,\n    model=model,\n    existing_set=existing_questions,\n    max_length=250,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.5,\n    max_tries=5\n):\n    \"\"\"\n    Generates a new question for the given 'topic' by prompting GPT-2 with:\n    \"Topic: {topic}\\nQuestion:\"\n    Skips if the question is exactly or too similar to existing dataset questions.\n    Tries up to 'max_tries'.\n    \"\"\"\n    for attempt in range(max_tries):\n        # 1) Prepare prompt\n        prompt = f\"Topic: {topic}\\nQuestion:\"\n        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n        input_ids = input_ids.to(model.device)\n\n        # 2) Generate\n        output = model.generate(\n            input_ids=input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            num_return_sequences=1\n        )\n\n        text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # 3) Extract the question portion\n        if \"Question:\" in text:\n            splitted = text.split(\"Question:\")\n            gen_question = splitted[-1].strip()\n        else:\n            gen_question = text.strip()\n\n        # # 4) Check for duplicates or near-duplicates\n        # if gen_question in existing_set:\n        #     print(\"Regenerating (exact match in dataset)...\")\n        #     continue\n        # if is_too_similar(gen_question, existing_set, threshold=fuzzy_threshold):\n        #     print(\"Regenerating (too similar to dataset)...\")\n        #     continue\n\n        # If we get here, it's a new question\n        return gen_question\n\n    return \"No unique question found after multiple attempts.\"\n\n\nnew_question = generate_unique_question_for_topic(\n    topic=predicted_topic,\n    max_length=250,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.4,\n    max_tries=5\n)\nprint(f\"For topic: '{predicted_topic}'\\nNew question:\\n{new_question}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:53:29.562354Z","iopub.execute_input":"2025-04-13T18:53:29.562614Z","iopub.status.idle":"2025-04-13T18:55:26.516362Z","shell.execute_reply.started":"2025-04-13T18:53:29.562594Z","shell.execute_reply":"2025-04-13T18:55:26.515464Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement difflib2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for difflib2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"950bc5579e45414b8bcc67c69bf6493d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd05b7ddfa0433fb9e125c295f007af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca53c799610142a6a060fefd6932b40a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb881068c64b4048a11c751bfe588509"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40810d9bd32b4163bcde929501873d2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369bde65f95d40aba7f19c5f8e19422f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cab160f9c1d5416e872021c8ced0ecae"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 01:21, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>4.493000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.023100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.880700</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.701600</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.567000</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.500200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.416100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.446600</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.350000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.326900</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.335200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.312800</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.274800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.252600</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.286200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.218800</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.241400</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.237800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.206900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.185900</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.225100</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.190400</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.192700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.175100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.179800</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.173700</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.147100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.133800</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.143700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.158300</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.127100</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.128800</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.140900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.130600</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.131200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.115400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.111200</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.115600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.109300</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.095800</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.113400</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.089900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.108600</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.106000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.099200</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.098500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.093400</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.090100</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.097300</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.090300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.095300</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.086700</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.082100</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.086500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.076600</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.089600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.085900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.074700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"Training completed.\nFor topic: '['the importance of his actions as examples for Muslim individuals in their personal conduct and relations with others including women and non-Muslims']'\nNew question:\n(a) The Prophet Muhammad (pbuh) was generous and just and forgiving. Give an account of the way in which this generosity can be shown in his life. (b) Giving examples from his life, write about how generosity can be shown in the lives of individuals today\n","output_type":"stream"}],"execution_count":2}]}