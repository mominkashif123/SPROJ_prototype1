{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11391578,"sourceType":"datasetVersion","datasetId":7134075},{"sourceId":11392137,"sourceType":"datasetVersion","datasetId":7134494}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport joblib  # for saving/loading models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.compose import ColumnTransformer\n\n# 1. Load Data\ndf = pd.read_csv(\"/kaggle/input/generation-dataset/Q4_dataset_complete.csv\")\n\n# Identify and remove rare classes (classes with only one sample)\nclass_counts = df['Q4_Topic'].value_counts()\nrare_classes = class_counts[class_counts == 1].index\ndf = df[~df['Q4_Topic'].isin(rare_classes)]\n\n# 2. Feature Engineering\nlabel_encoder = LabelEncoder()\ndf['Q4_Topic'] = df['Q4_Topic'].astype(str)\ndf['Q4_Topic_Encoded'] = label_encoder.fit_transform(df['Q4_Topic'])\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(handle_unknown='ignore'), ['Paper_Session'])],\n    remainder='passthrough'\n)\n\nX = df[['Year', 'Paper_Session', 'Paper_Varient']]\nX.columns = X.columns.astype(str)\nX_transformed = ct.fit_transform(X)  # fit_transform for training data\ny = df['Q4_Topic_Encoded']\n\n# 3. Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.3, random_state=42, stratify=y\n)\n\n# 4. Model Training\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Evaluation\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(classification_report(y_test, y_pred, zero_division=0))\n\n# 6. Prediction for Next Year (example)\nnext_year_data = pd.DataFrame({\n    'Year': [2025],\n    'Paper_Session': ['MJ'],\n    'Paper_Varient': ['12']\n})\n# We must transform it with the same ColumnTransformer\nX_next_year = ct.transform(next_year_data)\npredicted_topic_encoded = model.predict(X_next_year)\npredicted_topic = label_encoder.inverse_transform(predicted_topic_encoded)\nprint(f\"Predicted Topic for 2025: {predicted_topic[0]}\")\n\n# 7. SAVE EVERYTHING\n# We'll save: the trained model, the column transformer, and the label encoder\njoblib.dump(model, \"rf_model.pkl\")            # saves the RandomForest\njoblib.dump(ct, \"column_transformer.pkl\")     # saves the ColumnTransformer\njoblib.dump(label_encoder, \"label_encoder.pkl\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:56:55.505012Z","iopub.execute_input":"2025-04-13T18:56:55.505336Z","iopub.status.idle":"2025-04-13T18:56:55.726863Z","shell.execute_reply.started":"2025-04-13T18:56:55.505315Z","shell.execute_reply":"2025-04-13T18:56:55.726190Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.167\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         1\n           1       0.00      0.00      0.00         0\n           2       0.00      0.00      0.00         1\n           3       0.29      0.40      0.33         5\n           4       0.00      0.00      0.00         1\n           5       0.00      0.00      0.00         1\n           6       0.33      0.33      0.33         3\n           7       0.00      0.00      0.00         3\n           8       0.00      0.00      0.00         1\n           9       0.00      0.00      0.00         2\n\n    accuracy                           0.17        18\n   macro avg       0.06      0.07      0.07        18\nweighted avg       0.13      0.17      0.15        18\n\nPredicted Topic for 2025: the main events of his activities in Madina, his leadership of the community there and his conflicts with the Makkans and others\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['label_encoder.pkl']"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"!pip install transformers datasets imbalanced-learn difflib2 --quiet\n\nimport pandas as pd\nimport torch\nimport difflib\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    GPT2LMHeadModel,\n    GPT2Tokenizer,\n    Trainer,\n    TrainingArguments\n)\n\n#############################\n# 1) Load & Combine Dataset\n#############################\ndf = pd.read_csv(\"/kaggle/input/paper1-dataset/Q4_prediction_dataset.csv\")\ndf = df.dropna(subset=[\"Q4_Topic\", \"Q4\"])  # remove any empty rows\n\n# We'll store all existing questions in a set for uniqueness checks\nexisting_questions = set(df[\"Q4\"].str.strip())\n\n# For GPT-2 training, combine topic and question into a single string\ntrain_texts = []\nfor _, row in df.iterrows():\n    topic_str = str(row[\"Q4_Topic\"]).strip()\n    question_str = str(row[\"Q4\"]).strip()\n    combined = f\"Topic: {topic_str}\\nQuestion: {question_str}\\n<|endoftext|>\"\n    train_texts.append(combined)\n\n#############################\n# 2) Custom Dataset\n#############################\nclass TopicQuestionDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_length=128):\n        self.encodings = []\n        for txt in texts:\n            enc = tokenizer(\n                txt,\n                truncation=True,\n                max_length=max_length,\n                padding=\"max_length\"\n            )\n            self.encodings.append(enc)\n\n    def __len__(self):\n        return len(self.encodings)\n\n    def __getitem__(self, idx):\n        item = self.encodings[idx]\n        return {\n            \"input_ids\": torch.tensor(item[\"input_ids\"]),\n            \"attention_mask\": torch.tensor(item[\"attention_mask\"])\n        }\n\n\nmodel_name = \"gpt2\"  # or \"distilgpt2\" for smaller\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# GPT-2 pad fix\ntokenizer.pad_token = tokenizer.eos_token\nmodel.config.pad_token_id = model.config.eos_token_id\n\n\ntrain_dataset = TopicQuestionDataset(train_texts, tokenizer, max_length=250)\n\ndef data_collator(batch):\n    input_ids = torch.stack([f[\"input_ids\"] for f in batch])\n    attention_mask = torch.stack([f[\"attention_mask\"] for f in batch])\n    labels = input_ids.clone()  # causal language modeling\n    return {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n\ntraining_args = TrainingArguments(\n    output_dir=\"./temp-output\",\n    overwrite_output_dir=True,\n    num_train_epochs=20,\n    per_device_train_batch_size=2,\n    logging_steps=5,\n    logging_strategy=\"steps\",\n    save_strategy=\"no\",   # no checkpoint saving\n    report_to=[]\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset\n)\n\n#############################\n# 6) Fine-Tune GPT-2\n#############################\ntrainer.train()\nprint(\"Training completed.\")\n\n\n# def is_too_similar(new_q, existing_set, threshold=0.8):\n#     \"\"\"\n#     Returns True if 'new_q' is >= threshold similarity\n#     with any question in 'existing_set'.\n#     Using difflib.SequenceMatcher ratio.\n#     \"\"\"\n#     for q in existing_set:\n#         ratio = difflib.SequenceMatcher(None, new_q, q).ratio()\n#         if ratio >= threshold:\n#             return True\n#     return False\n\ndef generate_unique_question_for_topic(\n    topic,\n    tokenizer=tokenizer,\n    model=model,\n    existing_set=existing_questions,\n    max_length=250,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.5,\n    max_tries=5\n):\n    \"\"\"\n    Generates a new question for the given 'topic' by prompting GPT-2 with:\n    \"Topic: {topic}\\nQuestion:\"\n    Skips if the question is exactly or too similar to existing dataset questions.\n    Tries up to 'max_tries'.\n    \"\"\"\n    for attempt in range(max_tries):\n        # 1) Prepare prompt\n        prompt = f\"Topic: {topic}\\nQuestion:\"\n        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n        input_ids = input_ids.to(model.device)\n\n        # 2) Generate\n        output = model.generate(\n            input_ids=input_ids,\n            max_length=max_length,\n            temperature=temperature,\n            top_p=top_p,\n            do_sample=True,\n            num_return_sequences=1\n        )\n\n        text = tokenizer.decode(output[0], skip_special_tokens=True)\n        \n        # 3) Extract the question portion\n        if \"Question:\" in text:\n            splitted = text.split(\"Question:\")\n            gen_question = splitted[-1].strip()\n        else:\n            gen_question = text.strip()\n\n    \n        return gen_question\n\n    return \"No unique question found after multiple attempts.\"\n\n\nnew_question = generate_unique_question_for_topic(\n    topic=predicted_topic,\n    max_length=250,\n    temperature=0.7,\n    top_p=0.9,\n    fuzzy_threshold=0.4,\n    max_tries=5\n)\nprint(f\"For topic: '{predicted_topic}'\\nNew question:\\n{new_question}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T19:00:50.068312Z","iopub.execute_input":"2025-04-13T19:00:50.068674Z","iopub.status.idle":"2025-04-13T19:02:19.462750Z","shell.execute_reply.started":"2025-04-13T19:00:50.068647Z","shell.execute_reply":"2025-04-13T19:02:19.461969Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement difflib2 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for difflib2\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 01:26, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>4.528100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.034000</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.794500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.627400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.546300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.520200</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.374700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.436400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.359800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.298100</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.345000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.282900</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.272800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.274100</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.253600</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.199500</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>0.239300</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>0.241000</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>0.190700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.181600</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>0.211500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>0.174400</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>0.161500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.174800</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>0.164400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.151500</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.142200</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.125000</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>0.145500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.144900</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.133200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.123600</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.120600</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.100500</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.136800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.107200</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.103100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.100900</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.089800</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>0.105300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.103800</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.090400</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.093200</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.093300</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.086600</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.092400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.089300</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.086400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.088800</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.085700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.080300</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.083100</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.084300</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.083000</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.072100</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.082600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.080800</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.082700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.076900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Training completed.\nFor topic: '['the main events of his activities in Madina, his leadership of the community there and his conflicts with the Makkans and others']'\nNew question:\n(a) Write about the main events in the life of Abu Talib, the Prophetâ€™s close friend.  (b) How can Muslims use the example of Abu Talib to support their own communities?\n","output_type":"stream"}],"execution_count":5}]}